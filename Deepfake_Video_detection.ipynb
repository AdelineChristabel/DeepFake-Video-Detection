{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFake Video Detection using EfficientNetB2 + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.utils import shuffle, resample, class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, GlobalAveragePooling2D, GlobalAveragePooling1D,\n",
    "    LayerNormalization, MultiHeadAttention\n",
    ")\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "FRAME_COUNT = 20\n",
    "IMAGE_SIZE = 224\n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extractor : CNN base model using EfficientNetB2 pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cnn_base = EfficientNetB2(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "cnn_model = Sequential([cnn_base, GlobalAveragePooling2D()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, max_frames=FRAME_COUNT):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
    "    for i in range(total_frames):\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if i in frame_indices:\n",
    "            frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            frame = preprocess_input(frame)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(folder_path):\n",
    "    X, y = [], []\n",
    "    for label, class_dir in enumerate([\"real\", \"fake\"]):\n",
    "        class_path = os.path.join(folder_path, class_dir)\n",
    "        video_files = os.listdir(class_path)\n",
    "        random.shuffle(video_files)\n",
    "        for video_file in tqdm(video_files, desc=f\"Loading {class_dir}\"):\n",
    "            video_path = os.path.join(class_path, video_file)\n",
    "            try:\n",
    "                frames = extract_frames(video_path)\n",
    "                if frames.shape[0] == FRAME_COUNT:\n",
    "                    features = cnn_model.predict(frames, verbose=0)\n",
    "                    X.append(features)\n",
    "                    y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_transformer_model(input_shape, num_classes=2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_encoder(inputs)\n",
    "    x = transformer_encoder(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_dataset(\"D:/DeepFake2.0/split_dataset_part2/train\")\n",
    "X_test, y_test = load_dataset(\"D:/DeepFake2.0/split_dataset_part2/test\")\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imbalance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_real = X_train[y_train == 0]\n",
    "X_fake = X_train[y_train == 1]\n",
    "X_real_upsampled, y_real_upsampled = resample(X_real, np.zeros(len(X_real)), replace=True, n_samples=len(X_fake), random_state=SEED)\n",
    "X_train = np.concatenate([X_fake, X_real_upsampled])\n",
    "y_train = np.concatenate([np.ones(len(X_fake)), y_real_upsampled])\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, K.floatx())\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = K.pow((1 - p_t), gamma)\n",
    "        return K.mean(-alpha_factor * modulating_factor * K.log(p_t), axis=-1)\n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {i: class_weights[i] for i in range(2)}\n",
    "\n",
    "model = build_transformer_model(input_shape=(FRAME_COUNT, cnn_model.output_shape[1]))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=binary_focal_loss(), metrics=[\"accuracy\"])\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(monitor=\"val_accuracy\", patience=2, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_schedule],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"deepfake_efficientnetb2_transformer_balanced_focal2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.77).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"REAL\", \"FAKE\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_video(video_path):\n",
    "    frames = extract_frames(video_path)\n",
    "    if frames.shape[0] != FRAME_COUNT:\n",
    "        print(\"Insufficient frames for inference.\")\n",
    "        return\n",
    "    features = cnn_model.predict(frames, verbose=0)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    model = load_model(\"deepfake_efficientnetb2_transformer_balanced_focal2.keras\", compile=False)\n",
    "    prediction = model.predict(features)[0][0]\n",
    "    label = \"REAL\" if prediction < 0.5 else \"FAKE\"\n",
    "    confidence = (1 - prediction) * 100 if label == \"REAL\" else prediction * 100\n",
    "    print(f\"Prediction: {label} ({confidence:.2f}% confidence)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
